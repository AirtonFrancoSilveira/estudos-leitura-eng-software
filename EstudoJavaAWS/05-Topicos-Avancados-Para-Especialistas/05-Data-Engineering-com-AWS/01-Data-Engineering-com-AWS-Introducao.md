# üîç Data Engineering com AWS - Uma Introdu√ß√£o para Desenvolvedores Java

## üìö **Objetivos de Aprendizagem**

-   ‚úÖ **Compreender o que √© Data Engineering** e por que ele √© relevante para um desenvolvedor backend.
-   ‚úÖ **Conhecer os principais servi√ßos do ecossistema de Analytics da AWS:** Kinesis, Glue, Athena, e S3 como Data Lake.
-   ‚úÖ **Entender o fluxo de um pipeline de dados moderno na AWS**, da ingest√£o √† consulta.
-   ‚úÖ **Escrever uma fun√ß√£o Lambda em Java** para realizar um processo de ETL (Extract, Transform, Load) simples.
-   ‚úÖ **Identificar oportunidades** para aplicar conceitos de Data Engineering em sistemas transacionais.

---

## üéØ **Conceito 1: Onde o Backend Encontra os Dados (Analytics)**

Como desenvolvedor Java, voc√™ √© um mestre na constru√ß√£o de **sistemas transacionais (OLTP - Online Transaction Processing)**. Seu foco √© em baixa lat√™ncia, alta consist√™ncia e no processamento de opera√ß√µes de neg√≥cio (criar pedido, atualizar usu√°rio, etc.).

O **Data Engineering** foca em **sistemas anal√≠ticos (OLAP - Online Analytical Processing)**. O objetivo aqui √© processar grandes volumes de dados para extrair insights, alimentar dashboards de Business Intelligence (BI) ou treinar modelos de Machine Learning. O foco √© em alto throughput e na capacidade de realizar queries complexas sobre dados hist√≥ricos.

**Por que isso importa para voc√™?**
-   Os dados gerados pelos seus microservi√ßos (logs, eventos, registros do banco) s√£o o **combust√≠vel** para os pipelines de dados.
-   Cada vez mais, funcionalidades de backend precisam consumir insights gerados por processos de analytics (ex: um sistema de recomenda√ß√£o).
-   Conhecer o b√°sico de Data Engineering permite que voc√™ construa servi√ßos que se integram de forma mais eficiente e inteligente com o ecossistema de dados da empresa, tornando seu perfil profissional muito mais completo.

---

## üéØ **Conceito 2: A Arquitetura de um Pipeline de Dados Moderno na AWS**

Um pipeline de dados t√≠pico na AWS pode ser dividido em quatro est√°gios principais:

1.  **Ingest√£o (Ingest):** Coletar os dados brutos de diversas fontes.
2.  **Armazenamento (Store):** Armazenar os dados de forma barata, dur√°vel e escal√°vel em um **Data Lake**.
3.  **Processamento e Cat√°logo (Process & Catalog):** Transformar, limpar e enriquecer os dados, e criar um cat√°logo para que eles possam ser descobertos e consultados.
4.  **An√°lise e Visualiza√ß√£o (Analyze & Visualize):** Executar queries ad-hoc, criar dashboards ou alimentar outras aplica√ß√µes.

**Mapeando para Servi√ßos AWS:**

| Est√°gio | Servi√ßo AWS Principal | O que ele faz |
| :--- | :--- | :--- |
| **Ingest√£o** | **Amazon Kinesis** | Captura e processa streams de dados em tempo real (logs, eventos de clique, dados de IoT). |
| **Armazenamento** | **Amazon S3** | O cora√ß√£o do Data Lake. Armazena virtualmente qualquer volume de dados em seu formato bruto (JSON, CSV, Parquet).|
| **Processamento**| **AWS Glue** (ETL) | Um servi√ßo de ETL serverless. Ele pode "rastrear" (crawl) seus dados no S3, inferir o schema, e executar jobs (em Spark ou Python) para transformar os dados. |
| **Cat√°logo** | **AWS Glue Data Catalog** | Um "√≠ndice" ou metareposit√≥rio dos seus dados. Ele armazena as defini√ß√µes das tabelas que apontam para os arquivos no S3. |
| **An√°lise** | **Amazon Athena** | Permite que voc√™ execute **queries SQL padr√£o diretamente nos arquivos do seu S3**, usando o schema do Glue Data Catalog. √â totalmente serverless - voc√™ paga apenas pelas queries que executa. |

![AWS Analytics Pipeline](https://d1.awsstatic.com/product-marketing/Big-Data/Analytics_Pipeline_Diagram_updated.b8364c3c3a105220c35593c17885994e0781e64d.png)

---

## üéØ **Conceito 3: Exemplo Pr√°tico - Criando um Mini-ETL com Lambda em Java**

Imagine que nosso servi√ßo de pedidos publica um evento `PedidoCriado` em um t√≥pico SNS. Queremos capturar esses eventos, enriquec√™-los com dados do cliente (chamando outro servi√ßo) e salv√°-los em um formato otimizado (Parquet) no nosso Data Lake no S3.

```xml
<!-- pom.xml - Depend√™ncias para o Handler Lambda -->
<dependency>
    <groupId>com.amazonaws</groupId>
    <artifactId>aws-lambda-java-core</artifactId>
    <version>1.2.1</version>
</dependency>
<dependency>
    <groupId>com.amazonaws</groupId>
    <artifactId>aws-lambda-java-events</artifactId>
    <version>3.11.0</version>
</dependency>
<!-- SDK do S3 -->
<dependency>
    <groupId>software.amazon.awssdk</groupId>
    <artifactId>s3</artifactId>
    <version>2.17.290</version>
</dependency>
<!-- Biblioteca para trabalhar com JSON -->
<dependency>
    <groupId>com.fasterxml.jackson.core</groupId>
    <artifactId>jackson-databind</artifactId>
    <version>2.13.4</version>
</dependency>
```

```java
// O Handler da nossa fun√ß√£o Lambda
import com.amazonaws.services.lambda.runtime.Context;
import com.amazonaws.services.lambda.runtime.RequestHandler;
import com.amazonaws.services.lambda.runtime.events.SNSEvent;
import software.amazon.awssdk.core.sync.RequestBody;
import software.amazon.awssdk.services.s3.S3Client;
import software.amazon.awssdk.services.s3.model.PutObjectRequest;

public class PedidoEtlHandler implements RequestHandler<SNSEvent, String> {

    private final S3Client s3Client = S3Client.builder().build();
    private final ObjectMapper objectMapper = new ObjectMapper();
    private final String BUCKET_NAME = System.getenv("DATA_LAKE_BUCKET");

    @Override
    public String handleRequest(SNSEvent event, Context context) {
        for (SNSEvent.SNSRecord record : event.getRecords()) {
            try {
                // 1. EXTRACT: Pega o dado bruto do evento SNS
                String jsonPayload = record.getSNS().getMessage();
                PedidoCriadoEvent pedidoEvent = objectMapper.readValue(jsonPayload, PedidoCriadoEvent.class);

                // 2. TRANSFORM: Enriquecer o evento
                // Em um caso real, chamaria o servi√ßo de cliente para pegar mais dados
                ClienteInfo clienteInfo = buscarDadosCliente(pedidoEvent.getClienteId());
                
                PedidoEnriquecido pedidoEnriquecido = new PedidoEnriquecido(pedidoEvent, clienteInfo);

                // Converte o objeto enriquecido para JSON (ou Parquet/Avro em um caso real)
                String outputPayload = objectMapper.writeValueAsString(pedidoEnriquecido);

                // 3. LOAD: Salva o dado transformado no Data Lake (S3)
                String key = String.format("pedidos/ano=%d/mes=%d/dia=%d/pedido-%s.json",
                        LocalDate.now().getYear(),
                        LocalDate.now().getMonthValue(),
                        LocalDate.now().getDayOfMonth(),
                        pedidoEvent.getPedidoId());

                PutObjectRequest putObjectRequest = PutObjectRequest.builder()
                        .bucket(BUCKET_NAME)
                        .key(key)
                        .build();

                s3Client.putObject(putObjectRequest, RequestBody.fromString(outputPayload));
                
                context.getLogger().log("Pedido " + pedidoEvent.getPedidoId() + " processado e salvo no S3.");

            } catch (Exception e) {
                context.getLogger().log("Erro ao processar registro: " + e.getMessage());
                // Em um caso real, enviar para uma DLQ
            }
        }
        return "Processamento conclu√≠do";
    }
    
    private ClienteInfo buscarDadosCliente(String clienteId) {
        // Simula√ß√£o de uma chamada a um microservi√ßo de Clientes
        return new ClienteInfo(clienteId, "Jo√£o da Silva", "S√£o Paulo");
    }
}
```

**O que este c√≥digo faz:**
-   √â acionado por um evento (SNS).
-   **Extrai** o dado do evento.
-   **Transforma** o dado (enriquecendo-o).
-   **Carrega** o resultado no S3, particionando os dados por data (uma pr√°tica essencial em Data Lakes para otimizar queries).

Uma vez que os arquivos est√£o no S3, podemos usar o **AWS Glue Crawler** para catalog√°-los e o **Amazon Athena** para consult√°-los com SQL, sem precisar de nenhum servidor ou banco de dados.

--- 